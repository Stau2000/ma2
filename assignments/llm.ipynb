{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**AI & Machine Learning (KAN-CINTO4003U) - Copenhagen Business School | Spring 2025**\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Part III: LLM\n",
    "\n",
    "Please see the description of the assignment in the README file (section 3) <br>\n",
    "**Guide notebook**: [guides/llm_guide.ipynb](guides/llm_guide.ipynb)\n",
    "\n",
    "\n",
    "***\n",
    "\n",
    "<br>\n",
    "\n",
    "* Note that you should report results using a classification report. \n",
    "\n",
    "* Also, remember to include some reflections on your results: how do they compare with the results from Part I, BoW?, and part II, BERT? Are there any hyperparameters or prompting techniques that are particularly important?\n",
    "\n",
    "* You should follow the steps given in the `llm_guide` notebook\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from decouple import Config, RepositoryEnv\n",
    "from ibm_watsonx_ai import APIClient\n",
    "from ibm_watsonx_ai import Credentials\n",
    "from ibm_watsonx_ai.foundation_models import ModelInference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded API Key: SfwS4sH7VccxJDsfU4WmgxHKTCEFXfrPMle9pwBHfGTb\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Correct relative path from assignments folder\n",
    "env_path = os.path.abspath(\"mas/ma2/.env\")  # Moves up one level to ma2\n",
    "config = Config(RepositoryEnv(env_path))\n",
    "\n",
    "WX_API_KEY = config(\"WX_API_KEY\")\n",
    "print(\"Loaded API Key:\", WX_API_KEY)\n",
    "\n",
    "# Set up credentials\n",
    "credentials = Credentials(\n",
    "    url=\"https://us-south.ml.cloud.ibm.com/\",\n",
    "    api_key=WX_API_KEY\n",
    ")\n",
    "\n",
    "# Initialize API client\n",
    "client = APIClient(\n",
    "    credentials=credentials,\n",
    "    project_id=\"77fa3fd6-5a4e-422f-bca4-3a2392ce6b70\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ModelInference(\n",
    "    api_client=client,\n",
    "    model_id=\"ibm/granite-13b-instruct-v2\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_id': 'ibm/granite-13b-instruct-v2',\n",
       " 'created_at': '2025-03-30T07:13:41.683Z',\n",
       " 'results': [{'generated_text': 'The first step is to decide what parts you want to use. You can start with a motherboard',\n",
       "   'generated_token_count': 20,\n",
       "   'input_token_count': 8,\n",
       "   'stop_reason': 'max_tokens'}],\n",
       " 'system': {'warnings': [{'message': \"The value of 'parameters.max_new_tokens' for this model was set to value 20\",\n",
       "    'id': 'unspecified_max_new_tokens',\n",
       "    'additional_properties': {'limit': 0,\n",
       "     'new_value': 20,\n",
       "     'parameter': 'parameters.max_new_tokens',\n",
       "     'value': 0}}]}}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"How do I build a gaming PC?\"\n",
    "generated_response = model.generate(prompt)\n",
    "\n",
    "generated_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+----------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| PARAMETER             | TYPE                                   | EXAMPLE VALUE                                                                                                                             |\n",
      "+=======================+========================================+===========================================================================================================================================+\n",
      "| decoding_method       | str, TextGenDecodingMethod, NoneType   | sample                                                                                                                                    |\n",
      "+-----------------------+----------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| length_penalty        | dict, TextGenLengthPenalty, NoneType   | {'decay_factor': 2.5, 'start_index': 5}                                                                                                   |\n",
      "+-----------------------+----------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| temperature           | float, NoneType                        | 0.5                                                                                                                                       |\n",
      "+-----------------------+----------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| top_p                 | float, NoneType                        | 0.2                                                                                                                                       |\n",
      "+-----------------------+----------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| top_k                 | int, NoneType                          | 1                                                                                                                                         |\n",
      "+-----------------------+----------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| random_seed           | int, NoneType                          | 33                                                                                                                                        |\n",
      "+-----------------------+----------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| repetition_penalty    | float, NoneType                        | 2                                                                                                                                         |\n",
      "+-----------------------+----------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| min_new_tokens        | int, NoneType                          | 50                                                                                                                                        |\n",
      "+-----------------------+----------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| max_new_tokens        | int, NoneType                          | 1000                                                                                                                                      |\n",
      "+-----------------------+----------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| stop_sequences        | list, NoneType                         | 200                                                                                                                                       |\n",
      "+-----------------------+----------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| time_limit            | int, NoneType                          | 600000                                                                                                                                    |\n",
      "+-----------------------+----------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| truncate_input_tokens | int, NoneType                          | 200                                                                                                                                       |\n",
      "+-----------------------+----------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| return_options        | dict, ReturnOptionProperties, NoneType | {'input_text': True, 'generated_tokens': True, 'input_tokens': True, 'token_logprobs': True, 'token_ranks': False, 'top_n_tokens': False} |\n",
      "+-----------------------+----------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| include_stop_sequence | bool, NoneType                         | True                                                                                                                                      |\n",
      "+-----------------------+----------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| prompt_variables      | dict, NoneType                         | {'doc_type': 'emails', 'entity_name': 'Golden Retail'}                                                                                    |\n",
      "+-----------------------+----------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "from ibm_watsonx_ai.foundation_models.schema import TextGenParameters\n",
    "\n",
    "TextGenParameters.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAMS = TextGenParameters(\n",
    "    temperature=0.8,      # Higher temperature means more randomness\n",
    "    max_new_tokens=500, # Maximum number of tokens to generate\n",
    "    min_new_tokens=200, # Minimum number of tokens to generate\n",
    ")\n",
    "\n",
    "model = ModelInference(\n",
    "    api_client=client,\n",
    "    model_id=\"ibm/granite-13b-instruct-v2\",\n",
    "    params=PARAMS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_id': 'ibm/granite-13b-instruct-v2',\n",
       " 'created_at': '2025-03-30T07:13:46.245Z',\n",
       " 'results': [{'generated_text': 'To build a gaming PC, you will need a motherboard, processor, graphics card, memory, storage, and a power supply. You will also need to install an operating system and any games you want to play. \\nBuilding a gaming PC is not difficult, but it does require some experience with computer hardware. If you are not comfortable assembling your own PC, you can purchase a pre-built gaming PC from many retailers. \\nOnce you have all of the components, installing them into a case is fairly straightforward. Be sure to read the instructions that come with your motherboard and CPU before beginning assembly. \\nIf you have any problems during the build process, there are many online resources available to help you. Once your PC is assembled, you will need to install an operating system (such as Windows 10) and any games you want to play. \\nIf you have any questions about building a gaming PC, there are many online forums where you can ask for help. ',\n",
       "   'generated_token_count': 201,\n",
       "   'input_token_count': 8,\n",
       "   'stop_reason': 'eos_token',\n",
       "   'seed': 1846697202}]}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = model.generate(prompt)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To build a gaming PC, you will need a motherboard, processor, graphics card, memory, storage, and a power supply. You will also need to install an operating system and any games you want to play. \n",
      "Building a gaming PC is not difficult, but it does require some experience with computer hardware. If you are not comfortable assembling your own PC, you can purchase a pre-built gaming PC from many retailers. \n",
      "Once you have all of the components, installing them into a case is fairly straightforward. Be sure to read the instructions that come with your motherboard and CPU before beginning assembly. \n",
      "If you have any problems during the build process, there are many online resources available to help you. Once your PC is assembled, you will need to install an operating system (such as Windows 10) and any games you want to play. \n",
      "If you have any questions about building a gaming PC, there are many online forums where you can ask for help. \n"
     ]
    }
   ],
   "source": [
    "print(response[\"results\"][0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using LLMs for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report \n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data\n",
    "\n",
    "We can load this data directly from [Hugging Face Datasets](https://huggingface.co/docs/datasets/) - The HuggingFace Hub- into a Pandas DataFrame. Pretty neat!\n",
    "\n",
    "**Note**: This cell will download the dataset and keep it in memory. If you run this cell multiple times, it will download the dataset multiple times.\n",
    "\n",
    "You are welcome to increase the `frac` parameter to load more data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflections\n",
    "\n",
    "We tried changing the system prompt to be more accurate and specific compared to the one given in the guide\n",
    "Results: This greatly changed the results for the worse, resulting in us sticking to the one given in the guide.\n",
    "\n",
    "Secondly we changed the parameters, which increased the scores 1%-2%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "splits = {'train': 'data/train-00000-of-00001.parquet', 'test': 'data/test-00000-of-00001.parquet'}\n",
    "# train = pd.read_parquet(\"hf://datasets/fancyzhx/ag_news/\" + splits[\"train\"])\n",
    "test = pd.read_parquet(\"hf://datasets/fancyzhx/ag_news/\" + splits[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(760, 2)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "label_map = {\n",
    "    0: 'World',\n",
    "    1: 'Sports',\n",
    "    2: 'Business',\n",
    "    3: 'Sci/Tech'\n",
    "}\n",
    "\n",
    "def preprocess(df: pd.DataFrame, frac : float = 1e-2, label_map : dict[int, str] = label_map, seed : int = 42) -> pd.DataFrame:\n",
    "\n",
    "\n",
    "    return  (\n",
    "        df\n",
    "        .assign(label=lambda x: x['label'].map(label_map))\n",
    "        [lambda df: df['label'].isin(label_map.values())]\n",
    "        .groupby('label')[[\"text\", \"label\"]]\n",
    "        .apply(lambda x: x.sample(frac=frac, random_state=seed))\n",
    "        .reset_index(drop=True)\n",
    "\n",
    "    )\n",
    "\n",
    "# train_df = preprocess(train, frac=0.01)\n",
    "test_df = preprocess(test, frac=0.1)\n",
    "\n",
    "# clear up some memory by deleting the original dataframes\n",
    "# del train\n",
    "del test\n",
    "\n",
    "test_df.shape # , train_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAMS = TextGenParameters(\n",
    "    temperature=0.3,          # Slightly increase randomness\n",
    "    top_k=20,                 # Broaden token selection\n",
    "    top_p=0.95,               # Allow more varied generation\n",
    "    max_new_tokens=50,        # Give the model more room to classify\n",
    "    stop_sequences=[\"\\n\"]     # Let it stop naturally\n",
    ")\n",
    "\n",
    "model = ModelInference(\n",
    "    api_client=client,\n",
    "    model_id=\"ibm/granite-13b-instruct-v2\",  # We could also try a larger model!\n",
    "    params=PARAMS\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a system prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"You task is to classify news stories into one of five categories\n",
    "\n",
    "CATEGORIES:\n",
    "{categories}\n",
    "\n",
    "TEXT:\n",
    "{text}\n",
    "\n",
    "Please assign the correct category to the text. Answer with the correct category and nothing else.\n",
    "\n",
    "Category:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 760/760 [03:45<00:00,  3.37it/s]\n"
     ]
    }
   ],
   "source": [
    "CATEGORIES = \"- \" + \"\\n- \".join(test_df[\"label\"].unique())  # Create a string with all categories\n",
    "\n",
    "predictions = []\n",
    "\n",
    "for text in tqdm(test_df[\"text\"]):\n",
    "\n",
    "    # format the prompt with the categories and the text\n",
    "    prompt = SYSTEM_PROMPT.format(categories=CATEGORIES, text=text)\n",
    "    \n",
    "    # generate the response from the model\n",
    "    response = model.generate(prompt)\n",
    "\n",
    "    # extract the generated text from the response\n",
    "    prediction = response[\"results\"][0][\"generated_text\"].strip()\n",
    "\n",
    "    # append the prediction to the list of predictions\n",
    "    predictions.append(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Business       0.54      0.92      0.68       190\n",
      "    Sci/Tech       0.90      0.36      0.52       190\n",
      "      Sports       0.96      0.91      0.93       190\n",
      "       World       0.82      0.79      0.80       190\n",
      "\n",
      "    accuracy                           0.74       760\n",
      "   macro avg       0.80      0.74      0.73       760\n",
      "weighted avg       0.80      0.74      0.73       760\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(test_df.label, predictions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiml25-ma2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
